setwd("E:/TrendMining/source_code/TrendMining")
install.packages(c("ggplot2", "stopwords", "tm"))
#** CLEAN YOUR ENVIRONMENT
#** You may want to clear your environment variables when starting a session. Saves from plenty of headache.
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:/TrendMining/source_code/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Zt5gfc9eSaW68FsAKjHXhg(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
set_api_key("5cd1321fa87640a65e146086a5cffa2b")
#alternatively you may store it a personal file somewhere else.
#source("C:/Users/mmantyla/Dropbox/Research - Publications/2017 Agile Book Chapter/Scripts/SetScopusApiKey.R")
#install.packages("ggplot2", dependencies = TRUE)
#install.packages("tm", dependencies = TRUE)
#install.packages("magrittr", dependencies = TRUE)
#install.packages("stopwords", dependencies = TRUE)
library("tm")
library("stopwords")
library("magrittr")
library("ggplot2")
#Creates a pdf-file
#May take a long time
#May not be human readable (for large files)
#EDIT this row
my_file <- "my_Scopus_devops_data.RData"
print(paste("Dendogram Cluster, my_file: ", my_file))
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
View(my_articles)
library("tm")
library("stopwords")
library("magrittr")
library("ggplot2")
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"myStopword1", "myStopword2")
my_stopwords
#A good is to remove more words that we do not care about
Abstract_clean = removeWords(my_articles$Abstract_clean, my_stopwords)
Title = removeWords(my_articles$Title, my_stopwords)
# Create corpus by appending title and abstract to character string
corpus = Corpus(VectorSource(paste(Title, Abstract_clean)))
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
freq.terms = findFreqTerms(dtm, lowfreq=50)
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
freq.terms = findFreqTerms(dtm, lowfreq=50)
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 500) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 1000) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 500) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"devops", "software","paper","development")
#A good is to remove more words that we do not care about
Abstract_clean = removeWords(my_articles$Abstract_clean, my_stopwords)
Title = removeWords(my_articles$Title, my_stopwords)
# Create corpus by appending title and abstract to character string
corpus = Corpus(VectorSource(paste(Title, Abstract_clean)))
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 500) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"devops", "software","paper")
#A good is to remove more words that we do not care about
Abstract_clean = removeWords(my_articles$Abstract_clean, my_stopwords)
Title = removeWords(my_articles$Title, my_stopwords)
# Create corpus by appending title and abstract to character string
corpus = Corpus(VectorSource(paste(Title, Abstract_clean)))
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 500) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 100) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 200) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 300) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 400) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 300) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 200) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 250) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 300) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 270) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
dist.mat <- dist(as.matrix(dtm))
#Hirarcical clustering http://www.r-tutor.com/gpu-computing/clustering/hierarchical-cluster-analysis
h <- hclust(dist.mat, method = "ward.D")
#Finally,  we plot to file. R-studio plotting view is not large enough.
my_pdf_file = my_temp_file
my_pdf_file = gsub("data/my", "data/pdf", my_pdf_file)
my_pdf_file = gsub(".RData", ".pdf", my_pdf_file)
print(paste("Printing the Cluster Dendogram file:", my_pdf_file))
print("Finished Dendogram Cluster")
pdf(my_pdf_file, width=40, height=15)
par(cex=0.7, mar=c(5, 8, 4, 1))
#Remember we removed stopwords from title so the variable Title no longer make sense
plot(h, labels = my_articles$Title, sub = "")
#when using smaller set e.g. 200 articles only
#plot(h, labels = my_articles$Title[1:200], sub = "")
dev.off()
#** CLEAN YOUR ENVIRONMENT
#** You may want to clear your environment variables when starting a session. Saves from plenty of headache.
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:/TrendMining/source_code/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Zt5gfc9eSaW68FsAKjHXhg(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
set_api_key("5cd1321fa87640a65e146086a5cffa2b")
#alternatively you may store it a personal file somewhere else.
#source("C:/Users/mmantyla/Dropbox/Research - Publications/2017 Agile Book Chapter/Scripts/SetScopusApiKey.R")
library("tm")
library("stopwords")
library("magrittr")
library("ggplot2")
#library(SnowballC)
#Creates a pdf-file
#May take a long time
#May not be human readable (for large files)
#EDIT this row
my_file <- "my_Scopus_devops_data.RData"
#my_DtmAndDendogramClusterFile = function(my_file) {
print(paste("Dendogram Cluster, my_file: ", my_file))
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"devops", "software","paper")
#A good is to remove more words that we do not care about
Abstract_clean = removeWords(my_articles$Abstract_clean, my_stopwords)
Title = removeWords(my_articles$Title, my_stopwords)
#-----/Removing unnecessary words----------------
# Create corpus by appending title and abstract to character string
corpus = Corpus(VectorSource(paste(Title, Abstract_clean)))
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
freq.terms = findFreqTerms(dtm, lowfreq=50)
#Is "and" or "are" meaningfull words? Lets get rid of them by setting stopwords=TRUE
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=my_stopwords, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
dist.mat <- dist(as.matrix(dtm))
#Hirarcical clustering http://www.r-tutor.com/gpu-computing/clustering/hierarchical-cluster-analysis
h <- hclust(dist.mat, method = "ward.D")
#Finally,  we plot to file. R-studio plotting view is not large enough.
my_pdf_file = my_temp_file
my_pdf_file = gsub("data/my", "data/pdf", my_pdf_file)
my_pdf_file = gsub(".RData", ".pdf", my_pdf_file)
print(paste("Printing the Cluster Dendogram file:", my_pdf_file))
print("Finished Dendogram Cluster")
pdf(my_pdf_file, width=40, height=15)
par(cex=0.7, mar=c(5, 8, 4, 1))
#Remember we removed stopwords from title so the variable Title no longer make sense
plot(h, labels = my_articles$Title, sub = "")
#when using smaller set e.g. 200 articles only
#plot(h, labels = my_articles$Title[1:200], sub = "")
dev.off()
#install.packages("ggplot2", dependencies = TRUE)
#install.packages("tm", dependencies = TRUE)
#install.packages("magrittr", dependencies = TRUE)
#install.packages("stopwords", dependencies = TRUE)
library("tm")
library("stopwords")
library("magrittr")
library("ggplot2")
#library(SnowballC)
#Creates a pdf-file
#May take a long time
#May not be human readable (for large files)
#EDIT this row
my_file <- "my_Scopus_devops_data.RData"
#my_DtmAndDendogramClusterFile = function(my_file) {
print(paste("Dendogram Cluster, my_file: ", my_file))
my_temp_file = paste(my_data_dir, "/", sep="")
my_temp_file = paste(my_temp_file, my_file, sep="")
load(my_temp_file)
#-----Removing unnecessary words----------------
#various stopword lists can be used https://cran.r-project.org/web/packages/stopwords/stopwords.pdf
#stopword list is also context specific. Here you can do manual removals
#also automated methods tf/idf exist. EDIT
my_stopwords = c(stopwords::stopwords(language = "en", source = "snowball"),"devops", "software","paper")
#A good is to remove more words that we do not care about
Abstract_clean = removeWords(my_articles$Abstract_clean, my_stopwords)
Title = removeWords(my_articles$Title, my_stopwords)
#-----/Removing unnecessary words----------------
# Create corpus by appending title and abstract to character string
corpus = Corpus(VectorSource(paste(Title, Abstract_clean)))
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=FALSE, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
freq.terms = findFreqTerms(dtm, lowfreq=50)
#Is "and" or "are" meaningfull words? Lets get rid of them by setting stopwords=TRUE
dtm = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=FALSE,
stopwords=my_stopwords, wordLengths=c(3, Inf), removeNumbers=TRUE,
removePunctuation=TRUE, bounds=list(global=c(5,Inf))))
#Stemming can also be used but the results might not be any better.
dtm_stem = DocumentTermMatrix(corpus, control=list(tolower=TRUE, stemming=TRUE,
stopwords = c (stopwords("english"), my_stopwords),
wordLengths=c(3, Inf), removeNumbers=TRUE, removePunctuation=TRUE,
bounds=list(global=c(5,Inf)) ))
freq.terms = findFreqTerms(dtm_stem, lowfreq=50)
#Lets plot
#Convert to martix, compute colSums (total word counds), take only words with more than 500 occurecents,
#and convert to dataframe with two columns: terms and frequencies
#if too many or too little words showup EDIT number 500 accordingly
df <- dtm %>% as.matrix %>% colSums %>% subset (. >= 270) %>% data.frame(term=names(.), freq=.)
#do you see any new stopwords that could be added to the list
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") +xlab("Terms") + ylab("Count") + coord_flip()
#--------------------Cluster and and Plot dendogram ---------------------------------------
#We can subset to  200 papers if we have thounsands
#dtm <- dtm[1:200,]
# Compute distance matrix https://stat.ethz.ch/R-manual/R-devel/library/stats/html/dist.html
#https://en.wikipedia.org/wiki/Distance_matrix
#will be slow with thousands
dist.mat <- dist(as.matrix(dtm))
#Hirarcical clustering http://www.r-tutor.com/gpu-computing/clustering/hierarchical-cluster-analysis
h <- hclust(dist.mat, method = "ward.D")
#Finally,  we plot to file. R-studio plotting view is not large enough.
my_pdf_file = my_temp_file
my_pdf_file = gsub("data/my", "data/pdf", my_pdf_file)
my_pdf_file = gsub(".RData", ".pdf", my_pdf_file)
print(paste("Printing the Cluster Dendogram file:", my_pdf_file))
print("Finished Dendogram Cluster")
pdf(my_pdf_file, width=40, height=15)
par(cex=0.7, mar=c(5, 8, 4, 1))
#Remember we removed stopwords from title so the variable Title no longer make sense
plot(h, labels = my_articles$Title, sub = "")
#when using smaller set e.g. 200 articles only
#plot(h, labels = my_articles$Title[1:200], sub = "")
dev.off()
#}
