setwd("E:/University of Oulu/OneDrive - Oulun yliopisto/Next Generation Software Engineering/SourceCode/TrendMining")
#install.packages("text2vec", dependencies = TRUE)
library("text2vec")
install.packages("text2vec", dependencies = TRUE)
library("text2vec")
source("FunctionsScopusApi.R")
install.packages("rscopus")
#** CLEAN YOUR ENVIRONMENT
#** You may want to clear your environment variables when starting a session. Saves from plenty of headache.
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:\University of Oulu\OneDrive - Oulun yliopisto\Next Generation Software Engineering\SourceCode\TrendMining")
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:\TrendMining")
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:/University of Oulu\OneDrive - Oulun yliopisto/Next Generation Software Engineering/SourceCode/TrendMining")
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Zt5gfc9eSaW68FsAKjHXhg(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
set_api_key("5cd1321fa87640a65e146086a5cffa2b")
set_api_key("5cd1321fa87640a65e146086a5cffa2b")
source("FunctionsScopusApi.R")
setwd("E:/TrendMining/source_code/TrendMining")
rm(list=ls())
#** Set your work directory to the TrendMining project directory (where the script file are)
#** A folder "data" will be created for saving files (if such folder does not exist)
#** EDIT THE FOLLOWING LINE, set your own work directory
#setwd("K:/My Documents/Projects/TrendMining_2017/TrendMining")
setwd("E:/TrendMining/source_code/TrendMining")
my_work_dir = getwd()
my_data_dir = "data"
if (!file.exists(my_data_dir)) {
dir.create(file.path(my_work_dir, my_data_dir))
}
#** STACKOVERFLOW API KEY
#** Set your own StackOverflow API key here (or use the default below)
#** EDIT THE FOLLOWING LINE for your own API key
#api_key = "9raZ36FkYGFHDSNrW)gdsw((" TODO old file name edit out
so_api_key = "Zt5gfc9eSaW68FsAKjHXhg(("
#** GETOLDTWEETS-JAVA PATH
#** Set path to the directory for "GetOldTweets-java-master"
getoldtweets_path = paste(getwd(),"/GetOldTweets-java-master", sep="")
#** SCOPUS API KEY
#** Set your own Scopus API key here
#** Create an account & create your API key => <your-own-scopus-api-key>
#** https://dev.elsevier.com/user/login
#** Replace the next line with set_api_key("YOUR_SCOPUS_KEY_HERE")
#** EDIT THE FOLLOWING LINE with YOUR OWN Scopus API key
#install.packages("rscopus", dependencies = TRUE)
library("rscopus")
set_api_key("5cd1321fa87640a65e146086a5cffa2b")
#alternatively you may store it a personal file somewhere else.
#install.packages("text2vec", dependencies = TRUE)
library("text2vec")
source("FunctionsScopusApi.R")
#For example
#Finds 321 papers (29 April 2018). Suitable for classroom demo
query_string = "Continuous Integration"
my_filename = "ci"
my_query_string = "TITLE-ABS-KEY(\""
my_query_string = paste(my_query_string, query_string, sep="")
#EDIT this line
my_query_string = paste(my_query_string, "\") AND ALL('software testing')", sep="")
#Get articles and save those - we do not want to re-run the query
my_articles = get_scopus_papers(my_query_string)
abstract = my_articles$Abstract
abstract = gsub("Copyright ?+[^.]*[.]","",abstract)
abstract = gsub("?+[^.]*[.]","",abstract) # Depdenging on the enviroment or data you might need something different*
abstract = gsub("All rights reserved[.]","",abstract)
abstract = gsub("All right reserved[.]","",abstract)
abstract = gsub("No abstract available[.]","",abstract)
abstract = gsub("[0-9]", "", abstract)
#Get articles and save those - we do not want to re-run the query
my_articles = get_scopus_papers(my_query_string)
#For example
#Finds 321 papers (29 April 2018). Suitable for classroom demo
query_string = '((SRCTITLE("UbiComp") OR CONF ("UbiComp") OR CONFNAME ("UbiComp")) OR
(SRCTITLE("International Conference on Pervasive Computing ") OR CONF
("UbInternational Conference on Pervasive Computing iComp") OR CONFNAME
("International Conference on Pervasive Computing "))) AND NOT
(SRCTITLE("PervasiveHealth") OR CONF ("PervasiveHealth") OR CONFNAME
("PervasiveHealth"))'
my_filename = "uc"
my_query_string = "TITLE-ABS-KEY(\""
my_query_string = paste(my_query_string, query_string, sep="")
#EDIT this line
my_query_string = paste(my_query_string, "\") ", sep="")
#Get articles and save those - we do not want to re-run the query
my_articles = get_scopus_papers(my_query_string)
my_query_string = "TITLE-ABS-KEY(\""
my_query_string = paste(my_query_string, query_string, sep="")
#EDIT this line
my_query_string = paste(my_query_string, "\") ", sep="")
#For example
#Finds 321 papers (29 April 2018). Suitable for classroom demo
query_string = 'UbiComp'
my_filename = "uc"
my_query_string = "TITLE-ABS-KEY(\""
my_query_string = paste(my_query_string, query_string, sep="")
#EDIT this line
my_query_string = paste(my_query_string, "\") ", sep="")
#Get articles and save those - we do not want to re-run the query
my_articles = get_scopus_papers(my_query_string)
source('E:/TrendMining/source_code/TrendMining/GetScopusData.R')
source('E:/TrendMining/source_code/TrendMining/GetScopusData.R')
source("FunctionsStackOverflowApi.R")
install.packages("httr", dependencies = TRUE)
install.packages("xml2", dependencies = TRUE)
install.packages("urltools", dependencies = TRUE)
install.packages("httr", dependencies = TRUE)
source("FunctionsStackOverflowApi.R")
install.packages("urltools", dependencies = TRUE)
source("FunctionsStackOverflowApi.R")
install.packages("anytime", dependencies = TRUE)
source("FunctionsStackOverflowApi.R")
#For example
query_string = "UbiComp"#2208 items works for demo
my_filename = "uc_so"
my_articles = get_stackoverflow_data(query_string)
View(my_articles)
View(my_articles)
#For example
query_string = "Mining Software Repositories"#2208 items works for demo
my_filename = "uc_so"
my_filename = "msr_so"
my_articles = get_stackoverflow_data(query_string)
View(my_articles)
#For example
query_string = "Agile"#2208 items works for demo
my_filename = "agile_so"
my_articles = get_stackoverflow_data(query_string)
View(my_articles)
abstract = my_articles$Abstract
abstract = gsub("<code.*/code>", "", abstract)
abstract = gsub("<code.*<truncated>", "", abstract)
abstract = gsub("<.*?>", "", abstract)
abstract = gsub("//.*\n", " ", abstract)
abstract = gsub("\\{\n.*\\}\n", " ", abstract)
abstract = gsub("[\r\n]", " ", abstract)
abstract = gsub("\"", "", abstract)
abstract = gsub("[0-9]", "", abstract)
#Add cleaned abstracts as a new column.
#We could also replace the existing but debugging is easier if we keep both.
my_articles$Abstract_clean = tolower(abstract)
my_articles$Title = tolower(my_articles$Title)
#Date is character covert to Date object
my_articles$Date = as.Date(my_articles$Date)
my_articles$CR_Date = as.Date(my_articles$CR_Date)
my_articles$LA_Date = as.Date(my_articles$LA_Date)
#Fixed filename: data/my_STO_<xxx>_data.RData
my_file = my_work_dir
my_file = paste(my_file, "/data/my_STO_", sep="", collapse=" ")
my_file = paste(my_file, my_filename, sep="", collapse=" ")
my_file = paste(my_file, "_data.RData", sep="", collapse=" ")
save(my_articles, file=my_file)
return(my_file)
load("E:/TrendMining/source_code/TrendMining/data/my_STO_agile_so_data.RData")
#For example
query_string = "devops"#2208 items works for demo
my_filename = "devops_so"
my_articles = get_stackoverflow_data(query_string)
install.packages("rJava")
#install.packages("rJava", dependencies = TRUE)
library(rJava)
source("FunctionsTwitterApi.R")
install.packages("devtools")
source("FunctionsTwitterApi.R")
source("FunctionsTwitterApi.R")
#For example
query_string = "#devops"
my_filename = "devops"
#This may take quite a long time, depending on the data
#You may test duration like this. Then compute how long it would take to get max tweets
#system.time(my_articles <- get_twitter_data(query_string, maxtweets=100))
#system.time(my_articles <- get_twitter_data(query_string, maxtweets=200))
my_articles <- get_twitter_data(query_string, maxtweets=6000)
#save(my_articles, file="data/my_Twitter_articles_dirty.RData")
if (is.factor(my_articles$Abstract))
my_articles$Abstract = levels(my_articles$Abstract)[my_articles$Abstract]
abstract = my_articles$Abstract
title <- my_articles$Title
#Hashtags
abstract = gsub("#", " ", abstract)
abstract = gsub("(http|https)[://][^ ]*", " ", abstract)
abstract = gsub("@.*? ", " ", abstract)
abstract = gsub("@.*", " ", abstract)
abstract = gsub("[[:punct:]]", " ", abstract)
abstract = gsub("[\'\"/.,-:;!=%~*]", " ", abstract)
abstract = gsub("[.]", " ", abstract)
abstract = gsub("[ \t]{2,}", " ", abstract)
abstract <- chartr("åäáàâãöóòôõúùûüéèíìïëêñý", "aaaaaaooooouuuueeiiieeny", abstract)
#Text
title = gsub("#", " ", title)
title = gsub("(http|https)[://][^ ]*"," ",title)
title = gsub("@.*? ", " ", title)
title = gsub("@.*", " ", title)
title = gsub("[[:punct:]]", " ", title)
title = gsub("[\'\"/.,-:;!=%~*]", " ", title)
title = gsub("[.]", " ", title)
title = gsub("[ \t]{2,}", " ", title)
title <- chartr("åäáàâãöóòôõúùûüéèíìïëêñý", "aaaaaaooooouuuueeiiieeny", title)
if (is.factor(my_articles$AuthorName))
my_articles$AuthorName = levels(my_articles$AuthorName)[my_articles$AuthorName]
if (is.factor(my_articles$Cites)) {
my_articles$Cites = levels(my_articles$Cites)[my_articles$Cites]
my_articles$Cites = as.numeric(my_articles$Cites)
my_articles$Cites[is.na(my_articles$Cites)] = 0
}
if (is.factor(my_articles$Id)){
my_articles$Id = levels(my_articles$Id)[my_articles$Id]
my_articles$Id = as.numeric(my_articles$Id)
my_articles$Id[is.na(my_articles$Id)] = 0
}
#Add cleaned abstracts as a new column.
#We could also replace the existing but debugging is easier if we keep both.
my_articles$Abstract_clean = tolower(abstract)
my_articles$Title = tolower(title)
#Date is character covert to Date objec
my_articles$Date = as.Date(my_articles$Date)
#Fixed filename: /data/my_twitter_<xxx>_data.RData
my_file = my_work_dir
my_file = paste(my_file, "/data/my_twitter_", sep="", collapse=" ")
my_file = paste(my_file, my_filename, sep="", collapse=" ")
my_file = paste(my_file, "_data.RData", sep="", collapse=" ")
save(my_articles, file=my_file)
